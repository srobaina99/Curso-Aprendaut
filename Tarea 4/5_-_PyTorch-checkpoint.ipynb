{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZbzGZ5nx93iY"
   },
   "source": [
    "## Módulo 5 - PyTorch: Redes Neuronales en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrgAvcoi93io"
   },
   "source": [
    "En este notebook presentaremos una breve introducción a las características principales de PyTorch, una biblioteca para deep learning en Python. Este módulo asume que el lector conoce NumPY, la biblioteca de análisis numérico básico en Python, y scikit-learn, la biblioteca de aprendizaje automático más popular en el lenguaje (la utilizaremos para funciones auxiliares de carga de datos y alguna tarea más, pero no para el aprendizaje propiamente dicho). \n",
    "\n",
    "\n",
    "Referencias:\n",
    "\n",
    "- El notebook está basado principalmente en el libro [Machine Learning with PyTorch and Scikit-Learn](https://github.com/rasbt/machine-learning-book), de Sebastian Raschka, Yuxi Liu y Vahid Mirjalili, en particular los capítulos 12 y 13\n",
    "- La documentación de PyTorch tienen un [tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html) mostrando cómo entrenar una red neuronal que clasifica prendas, entrenado sobre el dataset FashionMNIST\n",
    "- Para una introducción paso a paso al uso de PyTorch para DL, el curso de Sebastian Raschka [Deep Learning fundamentals](https://lightning.ai/courses/deep-learning-fundamentals/) es un material excelente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xzp0A71393iw",
    "outputId": "ffef87cd-75fa-43dc-ae5e-886d519cc31a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Torch: 2.0.1\n",
      "Versión de Numpy: 1.24.3\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "# Este notebook fue elaborado con la versión 2.0.1 de Torch y la versión 1.24.3 de NumPy\n",
    "print(\"Versión de Torch:\",torch.__version__)\n",
    "print(\"Versión de Numpy:\",np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zk1gZeXk93jY"
   },
   "source": [
    "## 1. Tensores\n",
    "\n",
    "Las estructuras básicas de PyTorch son los _tensores_. Los tensores son generalizaciones de los escalares, vectores, matrices, etc. Un escalar es un tensor de rango 0, un vector es un tensor de rango 1, y una matriz es un tensor de rango 2. Los tensores, sin embargo, pueden tener rangos arbitrariamente grandes.  Los tensores son parecidos a los arrays de PyTorch, pero están optimizados para las operaciones de diferenciación automática y pueden correr en GPUs (volveremos sobre esto).\n",
    "\n",
    "Los tensores pueden crearse directamente, o a partir de listas de Python o arrays de NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([3, 4, 5], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "t_a = torch.tensor([1,2,3])\n",
    "print(t_a)\n",
    "\n",
    "b = np.array([3,4,5], dtype=np.int32)\n",
    "t_b = torch.from_numpy(b)\n",
    "print(t_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos consultar las propiedades de un tensor, tales como tipo de datos, y su forma: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5752, 0.7417, 0.1909, 0.6708],\n",
      "        [0.0739, 0.3009, 0.0959, 0.5807],\n",
      "        [0.5639, 0.7001, 0.7714, 0.2216]])\n",
      "torch.Size([3, 4])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "t_c = torch.rand(3,4)\n",
    "print(t_c)\n",
    "print(t_c.shape)\n",
    "print(t_c.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tensores pueden dinámicamente cambiar su forma (i.e. cuántos valores tiene cada dimensión). PyTorch resuelve de forma \"inteligente\" estos cambios, siempre que se mantenga la cantidad de elementos. El siguiente ejemplo transforma un tensor que es un vector de 10 elementos, en una matriz de 2 filas por 5 columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t_d = torch.ones(10)\n",
    "t_d1 = t_d.reshape(2,5) # La convención para el orden es, igual que en numpy, fila/columna\n",
    "print(t_d)\n",
    "print(t_d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Operaciones con Tensores\n",
    "\n",
    "Igual que numpy permite operar con arrays, PyTorch utiliza prácticamente las mismas reglas y operaciones para realizar operaciones matemáticas sobre los tensores. \n",
    "\n",
    "El siguiente ejemplo multiplica elemento a elemento dos tensores de tamaño 2x3 (el primero, creado a partir de números aleatorios con una distribución uniforme en \\[0,1), y el segundo con una distribución normal de media 0 y desviación estándar 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4581, 0.4829],\n",
      "        [0.3125, 0.6150],\n",
      "        [0.2139, 0.4118],\n",
      "        [0.6938, 0.9693],\n",
      "        [0.6178, 0.3304]])\n",
      "tensor([[-1.0531,  0.9457],\n",
      "        [-0.8307, -0.3713],\n",
      "        [ 1.2137, -0.6678],\n",
      "        [-0.4038, -0.1300],\n",
      "        [ 0.7410, -0.1425]])\n",
      "tensor([[-0.4824,  0.4567],\n",
      "        [-0.2596, -0.2284],\n",
      "        [ 0.2597, -0.2750],\n",
      "        [-0.2802, -0.1260],\n",
      "        [ 0.4578, -0.0471]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10) # Es buena costumbre fijar la semilla para que los resultados sean reproducibles\n",
    "t1 = torch.rand(5,2) \n",
    "t2 = torch.normal(mean=0,std=1,size=(5,2))\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "\n",
    "print(torch.multiply(t1,t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos también hacer cálculos sobre ciertas axes (axis=0 son las filas, axis=1 las columnas, y así sucesivamente). Por ejamplo, para calcular la media de cada una de las columnas del segundo tensor con números aleatorios, utilizamos `torch.mean`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0666, -0.0732])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(t2,axis=0)) # Debería dar algo cercano a 0..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular multiplicaciones entre matrices, se utiliza `torch.matmul`, que realiza las operaciones [usuales](https://es.wikipedia.org/wiki/Multiplicaci%C3%B3n_de_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.tensor([[1,2],[3,4]])\n",
    "t5 = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "print(torch.matmul(t4,t5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos calcular la [norma](https://es.wikipedia.org/wiki/Norma_vectorial) de un tensor, utilizaremos `torch.linalg.norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8368)\n",
      "tensor(49.7960)\n"
     ]
    }
   ],
   "source": [
    "t6 = torch.rand(100)\n",
    "print(torch.linalg.norm(t6,ord=2))\n",
    "print(torch.linalg.norm(t6,ord=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga de datos\n",
    "\n",
    "Habiendo visto las operaciones básicas sobre vectores, empezaremos a ver cómo entrenar una red neuronal utilizando PyTorch. Pero antes veremos algunas funciones útiles que PyTorch provee para manejar datasets, especialmente en aquellos casos (la mayoría) donde no podemos guardar todo el dataset en memoria, y deberemos irlo recuperando en pedazos (o batchs) desde disco, así como realizar otras funciones de preprocesamiento antes del aprendizaje propiamente dicho.\n",
    "\n",
    "Lo primero que vamos a ver es cómo crear un `DataLoader` a partir de un tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "t = torch.rand(20)\n",
    "data_loader = DataLoader(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habiendo construido el dataset, podemos iterar fácilmente sobre él: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4809])\n",
      "tensor([0.0962])\n",
      "tensor([0.6643])\n",
      "tensor([0.7610])\n",
      "tensor([0.7991])\n",
      "tensor([0.8031])\n",
      "tensor([0.7717])\n",
      "tensor([0.6075])\n",
      "tensor([0.5334])\n",
      "tensor([0.7489])\n",
      "tensor([0.0050])\n",
      "tensor([0.7465])\n",
      "tensor([0.6390])\n",
      "tensor([0.6332])\n",
      "tensor([0.3612])\n",
      "tensor([0.6436])\n",
      "tensor([0.6184])\n",
      "tensor([0.0132])\n",
      "tensor([0.2650])\n",
      "tensor([0.7495])\n"
     ]
    }
   ],
   "source": [
    "for item in data_loader:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos especificar que los elementos se procesen en batchs de cierto tamaño: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: tensor([0.4809, 0.0962, 0.6643, 0.7610, 0.7991, 0.8031])\n",
      "batch 2: tensor([0.7717, 0.6075, 0.5334, 0.7489, 0.0050, 0.7465])\n",
      "batch 3: tensor([0.6390, 0.6332, 0.3612, 0.6436, 0.6184, 0.0132])\n",
      "batch 4: tensor([0.2650, 0.7495])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(t,batch_size=6,drop_last=False) # No descartamos el último batch si está incompleto\n",
    "for i, batch in enumerate(data_loader,1):\n",
    "    print(f'batch {i}:', batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy común que tengamos dos tensores que nos interese procesar al mismo tiempo. El caso típico es un tensor con las features, y otro con las etiquetas. Para eso, la clase `TensorDataset` nos permite manejarlos como una unidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0.6414, 0.8605, 0.0916])  y: tensor(0)\n",
      "x: tensor([0.3973, 0.8914, 0.7675])  y: tensor(1)\n",
      "x: tensor([0.0880, 0.4807, 0.5685])  y: tensor(2)\n",
      "x: tensor([0.1676, 0.8042, 0.5398])  y: tensor(3)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Creamos un mini dataset que tiene 4 elementos y 3 atributos por cada uno\n",
    "t_x = torch.rand([4,3],dtype=torch.float32)\n",
    "t_y = torch.arange(4)\n",
    "joint_dataset = TensorDataset(t_x,t_y)\n",
    "\n",
    "for example in joint_dataset:\n",
    "    print('x:', example[0], ' y:', example[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este dataset, podemos, por supuesto, crear un nuevo DataLoader. En este caso, vamos a especificarle, además, que cada vez que recorra el dataset, mezcle las instancias de forma aleatoria. Esto será muy útil cuando recorramos varias veces el conjunto de entrenamiento para entrenar la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "data_loader = DataLoader(dataset=joint_dataset, batch_size=2, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos una iteración..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: [tensor([[0.6414, 0.8605, 0.0916],\n",
      "        [0.0880, 0.4807, 0.5685]]), tensor([0, 2])]\n",
      "batch 2: [tensor([[0.3973, 0.8914, 0.7675],\n",
      "        [0.1676, 0.8042, 0.5398]]), tensor([1, 3])]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(data_loader,1):\n",
    "    print(f'batch {i}:', batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y luego otra, sobre el mismo dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: [tensor([[0.1676, 0.8042, 0.5398],\n",
      "        [0.0880, 0.4807, 0.5685]]), tensor([3, 2])]\n",
      "batch 2: [tensor([[0.3973, 0.8914, 0.7675],\n",
      "        [0.6414, 0.8605, 0.0916]]), tensor([1, 0])]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(data_loader,1):\n",
    "    print(f'batch {i}:', batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cargando datos con torchvision\n",
    "\n",
    "Los métodos de `Dataset` y `DataLoader` permite construir datasets de forma muy general, a partir de nuestros datos, utilizando PyTorch. Sin embargo, para los datasets más populares, existen bibliotecas como `TorchVision` o `TorchText` que ya los proveen. Veamos, por ejemplo, cómo importar el dataset [MNIST](http://yann.lecun.com/exdb/mnist/) de números escritos a mano, utilizando `torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "image_path = './'\n",
    "\n",
    "# Esto nos permite convertir las features de los píxeles en tensores y los normaliza al rango [0,1]\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Importamos los datasets\n",
    "# Las etiquetas son valores entre 0 y 9 para representar los dígitos.\n",
    "mnist_train_dataset = torchvision.datasets.MNIST(root=image_path, train=True, transform=transform, download=True)\n",
    "mnist_test_dataset  = torchvision.datasets.MNIST(root=image_path, train=False, transform=transform, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "torch.manual_seed(10)\n",
    "train_dl = DataLoader(mnist_train_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. El modelo de computación de PyTorch\n",
    "\n",
    "PyTorch permite construir _grafos de computación_ para derivar las relaciones entre los tensores, cuando se realizan operaciones entre ellos (una red neuronal no es más que una serie de cálculos sobre tensores, partiendo del tensor inicial con la entrada). Trabajaremos con el ejemplo $ z = 2x(a-b) +c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "def compute_z(a,b,c):\n",
    "    r1 = torch.sub(a,b)\n",
    "    r2 = torch.mul(r1,2)\n",
    "    z = torch.add(r2,c)\n",
    "    return z\n",
    "    \n",
    "c= compute_z(torch.tensor([1,2,3]),torch.tensor([2,2,3]),torch.tensor([1,1,1]))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `compute_z` permite obtener el resultado sobre tensores cualesquiera (lo cual podríamos haber hecho sin problemas con `NumPy`) sino que conoce la forma de computarlo... lo que le permitirá luego calcular automática y eficientemente el gradiente, fundamental para el algoritmo de backpropagation, para aquellos tensores donde los especifiquemos explícitamente.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1400, requires_grad=True)\n",
      "tensor(12., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(3.14, requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "b = torch.tensor(12.0, requires_grad=True)\n",
    "b.requires_grad_\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Diferenciación automática\n",
    "\n",
    "Sabemos que para entrenar una red neuronal necesitamos calcular los gradientes de la función de pérdida con respecto a los pesos de la red. Veremos que PyTorch calcula estos gradientes de forma dinámica, y luego veremos cómo implementar un modelo utilizando estas capacidades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw: tensor(-0.5600)\n",
      "dL/db: tensor(-0.4000)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0, requires_grad = True)\n",
    "b = torch.tensor(0.5, requires_grad = True)\n",
    "\n",
    "# z = wx + b\n",
    "x = torch.tensor([1.4])\n",
    "y = torch.tensor([2.1])\n",
    "z = torch.add(torch.mul(w,x),b)\n",
    "\n",
    "# mi función de pérdida es la pérdida cuadrática (y-z)^2\n",
    "loss = (y-z).pow(2).sum()\n",
    "\n",
    "# calculo el gradiente\n",
    "loss.backward()\n",
    "print('dL/dw:',w.grad)\n",
    "print('dL/db:',b.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el atributo `grad` de $w$ y de $b$ contiene el gradiente de la función de pérdida respecto al parámetro correspondiente. El método `backward()` es el que hace la magia de la backpropagation (y por eso debemos invocarlo luego de calcular la pérdida). Como en este caso la derivada es muy fácil de calcular ($2x(wx+b-y)$), podemos verificar que PyTorch está haciendo bien las cuentas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5600], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(2 * x * (( w*x +b) - y ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. nn.Sequential\n",
    "\n",
    "PyTorch provee una clase [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) que permite especificar redes feed forward de forma muy sencilla. Veamos un ejemplo\":\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(4,16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16,32),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red anterior recibe 4 reales como entrada y tiene dos capas: la primera tiene 16 nodos, y su función de activación es una ReLU, mientras que la segunda tiene 32 nodos y también una ReLU como salida. Por lo tanto, esta red tendrá como salida un valor real (y por lo tanto sería adecuada para un problema de regresión). Sobre este modelo, es posible especificar diferentes funciones de activación, aplicar regularizaciones, entre otras muchas funciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Definición y entrenamiento de una red neuronal\n",
    "\n",
    "A partir de los datos de los dígitos de MNIST que importamos previamente, vamos a intentar predecir con una red neuronal las clases correspondientes. \n",
    "\n",
    "Primero, construimos el modelo de red (seguimos el ejemplo del libro de Rashka mencionado en la introducción): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [32,16]\n",
    "image_size = mnist_train_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size [2]\n",
    "\n",
    "# Nuestro primer paso es convertir las imágenes  a un vector\n",
    "all_layers = [nn.Flatten()]\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "\n",
    "all_layers.append(nn.Linear(hidden_units[-1],10))\n",
    "\n",
    "model = nn.Sequential(*all_layers)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, realizamos el entrenamiento. Para esto, especificamos la función de pérdida (en esta caso, Entropía cruzada) y el optimizador (Adam). Nos intenersa usar entropía cruzada porque es la función de pérdida típica para softmax. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x154bb338070>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "torch.manual_seed(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos por 10 épocas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Accuracy 0.8552\n",
      "Epoch 1  Accuracy 0.9329\n",
      "Epoch 2  Accuracy 0.9472\n",
      "Epoch 3  Accuracy 0.9552\n",
      "Epoch 4  Accuracy 0.9600\n",
      "Epoch 5  Accuracy 0.9638\n",
      "Epoch 6  Accuracy 0.9676\n",
      "Epoch 7  Accuracy 0.9701\n",
      "Epoch 8  Accuracy 0.9719\n",
      "Epoch 9  Accuracy 0.9741\n",
      "Epoch 10  Accuracy 0.9753\n",
      "Epoch 11  Accuracy 0.9775\n",
      "Epoch 12  Accuracy 0.9783\n",
      "Epoch 13  Accuracy 0.9792\n",
      "Epoch 14  Accuracy 0.9809\n",
      "Epoch 15  Accuracy 0.9814\n",
      "Epoch 16  Accuracy 0.9824\n",
      "Epoch 17  Accuracy 0.9831\n",
      "Epoch 18  Accuracy 0.9835\n",
      "Epoch 19  Accuracy 0.9848\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    accuracy_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        # Paso forward, obtenemos la predicción de la red con los valores actuales\n",
    "        pred = model(x_batch)\n",
    "        # Calculamos la pérdida. \n",
    "        loss = loss_fn(pred,y_batch)\n",
    "        # Backpropagation!\n",
    "        loss.backward()\n",
    "        # Ajustamos los pesos utilizando el algoritmo de optimización\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        is_correct = (torch.argmax(pred,dim=1) == y_batch).float()\n",
    "        \n",
    "        accuracy_hist_train += is_correct.sum()\n",
    "        \n",
    "    accuracy_hist_train /= len(train_dl.dataset)\n",
    "    print(f'Epoch {epoch}  Accuracy '\n",
    "          f'{accuracy_hist_train:.4f}')\n",
    "          \n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro modelo logró una accuracy de 98.48% sobre el conjunto de entrenamiento. Veamos ahora cómo es su performance sobre el conjunto de evaluación, para verificar que no hubo sobreajuste..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print((mnist_test_dataset.data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9665\n"
     ]
    }
   ],
   "source": [
    "# Predecimos utilizando el modelo\n",
    "\n",
    "# Antes normalizamos los valores RGB\n",
    "# Esto lo hago porque no paso por el transform\n",
    "pred = model(mnist_test_dataset.data/255.)\n",
    "\n",
    "is_correct = ( torch.argmax(pred,dim=1) == mnist_test_dataset.targets).float()\n",
    "print(f'Test accuracy: {is_correct.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Más para leer\n",
    "\n",
    "En el capítulo 13 del libro explica (y hay código asociado) cómo utilizar la biblioteca Lightining Trainer y TensorBoard para mostrar mejor los resultados. El código está disponible en [github](https://github.com/rasbt/machine-learning-book/tree/main/ch13)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Intro_Pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
